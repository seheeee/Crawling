{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc896ad5",
   "metadata": {},
   "source": [
    "# 실전크롤링 - 1. 네이버 증권 뉴스\n",
    "- 2024/04/30 네이버 증권 주요뉴스의 [제목, 상세페이지링크, 내용, 언론사, 날짜]를 엑셀에 저장하기\n",
    "- 정적 크롤링 기초\n",
    "- 마지막 페이지 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea393cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://finance.naver.com/news/mainnews.naver'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501b481",
   "metadata": {},
   "source": [
    "## 1) 첫번째 페이지 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775701a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1단계 : 첫번째 기사 가져오기\n",
    "#2단계 : 첫번째 페이지 20개 가져오기\n",
    "tags = soup.select('.block1')\n",
    "for tag in tags:\n",
    "    title = tag.select_one('.articleSubject > a').text #텍스트만 추출하는 법?\n",
    "    link =  'https://finance.naver.com'  + tag.select_one('.articleSubject > a').attrs['href'] #링크 추출하는 법? > 전체로 추출\n",
    "    summary = tag.select_one('.articleSummary').contents[0].strip() #첫번째 텍스트만 추출하는 법? /t 제외하기\n",
    "    company = tag.select_one('.press').text.strip() #공백제거하기기\n",
    "    time = tag.select_one('.wdate').text\n",
    "    print('-------------', title, link, summary, company, time)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f861c8",
   "metadata": {},
   "source": [
    "## 2) 첫번째 ~ 마지막페이지 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a23e2d",
   "metadata": {},
   "source": [
    "### a. 나의풀이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3단계 : 첫번째 ~ 마지막페이지까지 가져오기\n",
    "# 나의 풀이\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://finance.naver.com/news/mainnews.naver'\n",
    "\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#마지막페이지 찾기\n",
    "last_page = soup.select_one('.pgRR > a').attrs['href']\n",
    "pages = int(last_page[-1])\n",
    "\n",
    "#페이지마다 크롤링\n",
    "output = pd.DataFrame(columns =['title','link','summary','company','time'])\n",
    "page=1\n",
    "i = 0\n",
    "for page in range(pages+1):\n",
    "    page_link = url + '?&page=' + str(page)\n",
    "\n",
    "    response = requests.get(page_link)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    tags = soup.select('.block1')\n",
    "    for tag in tags:\n",
    "        title = tag.select_one('.articleSubject > a').text #텍스트만 추출하는 법?\n",
    "        link =  'https://finance.naver.com'  + tag.select_one('.articleSubject > a').attrs['href'] #링크 추출하는 법? > 전체로 추출\n",
    "        summary = tag.select_one('.articleSummary').contents[0].strip() #첫번째 텍스트만 추출하는 법? /t 제외하기\n",
    "        company = tag.select_one('.press').text.strip() #공백제거하기\n",
    "        time = tag.select_one('.wdate').text\n",
    "        \n",
    "        output.loc[i,'title'] = title\n",
    "        output.loc[i,'link'] = link\n",
    "        output.loc[i,'summary'] = summary\n",
    "        output.loc[i,'company'] = company\n",
    "        output.loc[i,'time'] = time\n",
    "        i+=1\n",
    "        \n",
    "output.to_excel(f'C:/Users/qawse/세희/IT공부/크롤링_inflearn/output.xlsx', index=None)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008796d6",
   "metadata": {},
   "source": [
    "### b. 강의풀이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b57dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3단계 : 첫번째 ~ 마지막페이지까지 가져오기\n",
    "#> 맨 뒤 버튼의 유무를 확인하여 반복 멈추기 **\n",
    "# 리스트에 2차원 객체로 담아 한번에 df로 변경하기 **\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "output = []\n",
    "for i in range(1, 1000):\n",
    "    url = 'https://finance.naver.com/news/mainnews.naver?&page='+str(i)\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    tags = soup.select('.block1')\n",
    "    for tag in tags:\n",
    "        title = tag.select_one('.articleSubject > a').text #텍스트만 추출하는 법?\n",
    "        link =  'https://finance.naver.com'  + tag.select_one('.articleSubject > a').attrs['href'] #링크 추출하는 법? > 전체로 추출\n",
    "        summary = tag.select_one('.articleSummary').contents[0].strip() #첫번째 텍스트만 추출하는 법? /t 제외하기\n",
    "        company = tag.select_one('.press').text.strip() #공백제거하기\n",
    "        time = tag.select_one('.wdate').text\n",
    "        # print(title, link, summary, company, time)\n",
    "\n",
    "        output.append([title, link, summary, company, time])\n",
    "\n",
    "    if soup.select_one('.pgRR') == None: #없으면 None 객체가 반환된다.\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(output, columns = ['title','link','summary','company','time']) \n",
    "df.to_excel(f'C:/Users/qawse/세희/IT공부/크롤링_inflearn/naver_finance_crawling.xlsx', index=None)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
